{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics gradio"
      ],
      "metadata": {
        "id": "K5VUeCm4sR45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gradio with yolo, predict video object and label it, plot label rectangle\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load a pretrained YOLOv8n model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "def predict_video(video_path):\n",
        "    results = model.predict(source=video_path, save=True)\n",
        "    output_video_path = results[0].path\n",
        "\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(f'/tmp/output.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "\n",
        "      # Run YOLOv8 inference on the frame\n",
        "      results = model(frame)\n",
        "\n",
        "      annotated_frame = results[0].plot()\n",
        "\n",
        "      out.write(annotated_frame)\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    return f'/tmp/output.mp4'\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_video,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=gr.Video(),\n",
        "    title=\"YOLOv8 Video Object Detection\",\n",
        "    description=\"Upload a video and detect objects using YOLOv8.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "rpvXssrlr6w_",
        "outputId": "0456aa5c-93ed-4eb6-b94e-0b848a378786"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4a5793c1efa18e6bb9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4a5793c1efa18e6bb9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gradio with yolo lite version for mobile device, and label object, and plot rectangle for object\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load a pretrained YOLOv8n model (you can change this to a smaller model like yolov8n.pt)\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "def predict_video(video_path):\n",
        "    results = model.predict(source=video_path, save=True)\n",
        "    output_video_path = results[0].path\n",
        "\n",
        "    # Open the input video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(f'/tmp/output.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "\n",
        "      # Run YOLOv8 inference on the frame\n",
        "      results = model(frame)\n",
        "\n",
        "      # Plot the bounding boxes and labels\n",
        "      annotated_frame = results[0].plot()\n",
        "\n",
        "      out.write(annotated_frame)\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    return f'/tmp/output.mp4'\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict_video,\n",
        "    inputs=gr.Video(),\n",
        "    outputs=gr.Video(),\n",
        "    title=\"YOLOv8 Video Object Detection\",\n",
        "    description=\"Upload a video and detect objects using YOLOv8.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "D1sWpSZstFIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}